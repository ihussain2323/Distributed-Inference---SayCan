"""
SayCan Clean - Efficient distributed Llama inference with minimal token usage

This script provides clean, focused results without verbose output.
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), "Distributed Inference", "Ray Distributed"))

import ray
import torch
from dataclasses import dataclass
import json
import re

# Import the distributed inference functions
from llama_client import generate_text, wait_for_workers

@dataclass
class RobotAction:
    name: str
    coordinates: list
    score: float = 0.0
    reasoning: str = ""

# Default actions (fallback)
default_actions = [
    RobotAction("go to garage", [5.0, 0.0, 0.0]),
    RobotAction("open garage door", [5.0, 0.0, 0.0]),
    RobotAction("go to closet", [5.0, 2.0, 0.0]),
    RobotAction("open closet", [5.0, 2.0, 0.0]),
    RobotAction("search corner", [5.0, 2.5, 0.0]),
    RobotAction("pick up sports drink", [5.0, 2.3, 0.0]),
    RobotAction("return to base", [0.0, 0.0, 0.0]),
]

def generate_actions(goal: str):
    """Generate relevant actions with reasoning and coordinates based on goal."""
    action_prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are a robot planner. Given the goal, output 5-6 possible robot actions. For each action, output:
ActionName [x, y, z]: Reasoning (1 sentence)
Do not repeat the goal or instructions. Example:
MoveToKitchen [1.0, 2.0, 0.0]: Move to the kitchen to access the target area.
PickUpGlass [1.2, 2.1, 0.8]: Pick up the glass from the counter.
<|eot_id|><|start_header_id|>user<|end_header_id|>
Goal: {goal}
<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""
    try:
        response = generate_text(action_prompt, max_new_tokens=80)
        if "<|start_header_id|>assistant<|end_header_id|>" in response:
            actions_text = response.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
        else:
            actions_text = response
        actions = []
        lines = actions_text.split('\n')
        for line in lines:
            line = line.strip()
            if '[' in line and ']' in line and ':' in line:
                try:
                    name = line.split('[')[0].strip()
                    coords_str = line.split('[')[1].split(']')[0]
                    coords = [float(x.strip()) for x in coords_str.split(',')]
                    reasoning = line.split(':', 1)[1].strip()
                    if len(coords) == 3:
                        actions.append(RobotAction(name, coords, 0.0, reasoning))
                except:
                    continue
        if not actions:
            print("‚ö†Ô∏è Using default actions (AI parsing failed)")
            return default_actions
        actions.append(RobotAction("return to base", [0.0, 0.0, 0.0], 0.0, "Return to the starting position."))
        return actions
    except Exception as e:
        return default_actions

def calculate_affordance(coords):
    """Calculate affordance based on distance from robot."""
    distance = sum(x ** 2 for x in coords) ** 0.5
    return max(0.0, 1.0 - distance / 5.0)

def saycan_clean_planner(goal: str):
    """
    Clean SayCan planning with 5-step plan and action table.
    Uses distributed Llama workers for plan generation, but actions are hardcoded for stability.
    """
    print(f"üéØ Goal: {goal}")
    plan_lines = []
    try:
        import ray
        from llama_client import generate_text, wait_for_workers
        ray.init(namespace="llama_inference", ignore_reinit_error=True)
        if not wait_for_workers():
            print("‚ùå Workers not available, using fallback plan.")
            raise Exception("Workers not available")
        # LLM prompt for 5-step plan
        plan_prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are a robot planner. Given the goal, output exactly 5 steps, each starting with a number and a period (e.g., 1. ...). Do not repeat the goal or instructions, just the steps.<|eot_id|><|start_header_id|>user<|end_header_id|>
Goal: {goal}
<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""
        plan_response = generate_text(plan_prompt, max_new_tokens=80)
        print("\n[DEBUG] Raw LLM plan response:\n" + plan_response)
        # Extract lines that look like steps
        for line in plan_response.split('\n'):
            line = line.strip()
            if line and (re.match(r'^[1-5][\).]', line) or re.match(r'^[1-5]\. ', line)):
                plan_lines.append(line)
        # If not exactly 5, try to salvage
        if len(plan_lines) < 5:
            print(f"[WARN] LLM did not return 5 steps, got {len(plan_lines)}. Using first 5 plausible lines.")
            plausible = [l.strip() for l in plan_response.split('\n') if l.strip() and not l.lower().startswith(('system','user','assistant'))]
            plan_lines = plausible[:5]
        if len(plan_lines) < 5:
            raise Exception("LLM did not return enough steps")
        ray.shutdown()
    except Exception as e:
        # Fallback hardcoded plan
        plan_lines = [
            "1. Move to the kitchen.",
            "2. Locate the glass.",
            "3. Inspect the glass for ice.",
            "4. Gently pick up the glass.",
            "5. Return to the starting position."
        ]
        print(f"[WARN] Using fallback plan: {e}")
    print(f"\nüìã 5-Step Plan:")
    for step in plan_lines:
        print(step)
    # Hardcoded action analysis table
    print(f"\nüìä Action Analysis:")
    actions = [
        ("GoToKitchen", 0.8, [1.0, 2.0, 0.0]),
        ("PickUpGlass", 0.7, [1.2, 2.1, 0.8]),
        ("ReturnToBase", 1.0, [0.0, 0.0, 0.0]),
    ]
    best_action = max(actions, key=lambda a: a[1])
    for name, affordance, coords in actions:
        print(f"{name} | {affordance:.2f} | {coords}")
    print(f"\nüèÜ Best Action: {best_action[0]}")
    print(f"üìç Location: {best_action[2]}")
    print(f"üìà Affordance Score: {best_action[1]:.2f}")

def main():
    """Main function."""
    
    print("üöÄ SayCan Clean Test")
    print("=" * 30)
    
    # Initialize Ray
    try:
        ray.init(namespace="llama_inference")
        print("‚úÖ Connected to Ray")
    except Exception as e:
        print(f"‚ùå Ray connection failed: {e}")
        return
    
    # Wait for workers
    if not wait_for_workers():
        print("‚ùå Workers not available")
        ray.shutdown()
        return
    
    # Run planning
    goal = "Go to the hallway and grab my glasses from the table there."
    saycan_clean_planner(goal)
    
    # Cleanup
    ray.shutdown()
    print("\n‚úÖ Complete!")

if __name__ == "__main__":
    main() 